# Монте-Карло в задачах статистики {#sec-monte-carlo}

```{python}
#| include: false

with open("_common.py") as f:
    exec(f.read())
```

У цій частині ми розглянемо метод Монте-Карло, який є потужним інструментом для чисельного моделювання та статистичного аналізу. Метод Монте-Карло дозволяє оцінювати ймовірності, інтеграли та інші статистичні характеристики шляхом випадкового вибору з певного розподілу. Ключовим моментом, на котрому ми зупинимось, це пошук відповідей на питання:

- Як перевірити наш критерій?
- Чи можна використовувати критерій на практиці?
- Якщо у нас є дві або більше альтернатив, як обрати найкращу?

## Перевірка критерію {#sec-check-criterion}

За допомогою методу Монте-Карло ми в *загальному випадку* зможемо відповісти на запитання:

- Чи можна використовувати цей критерій для нашого завдання?
- Чи правильно взагалі реалізовано критерій?

Увесь цей розділ насамперед буде присвячено AB-тестам і як можна перевіряти критерії для них. Основним критерієм для перевірки в цьому розділі стане $t$-тест, оскільки навколо нього обертається доволі багато міфів та непорозумінь. Ми з вами:

- Покажемо на практиці, що $t$-тест працює для вибірок не тільки з нормального розподілу.
- Подивимося, як визначити, з якого розміру вибірки можна застосовувати $t$-тест.

Як ми пам'ятаємо з минулої глави (див. -@sec-t-test), $t$-тест працює теоретично для вибірок з будь-якого розподілу, якщо вибірка досить велика. Але що значить, що критерій "коректний"? Давайте підемо від визначення:

- Критерій рівня значущості $\alpha$ означає, що ймовірність невірно відкинути нульову гіпотезу $\le \alpha$.
- А це зі свого боку означає, що якщо нескінченно багато разів повторити один й той самий експеримент, у якому правильна нульова гіпотеза, генеруючи наново експеримент, то кількість хибнопозитивних спрацьовувань буде меншою за $\alpha$ відсотків.

Ці визначення дозволяють нам визначити процедуру перевірки критерію:

1. Створюємо код критерію, який ми будемо перевіряти.
2. Генеруємо якомога більше експериментів, де вірна $H_0$.
3. Досліджуємо на них придуманий критерій.
4. Перевіряємо, чи правда, що тільки в $\alpha$ відсотків випадків критерій відкидається?

Тепер давайте розглянемо процедуру більш детально:

1. Насамперед треба вибрати розподіл, який буде описувати наші дані. Наприклад, якщо у нас метрика конверсії, то це розподіл Бернуллі, а якщо метрика --- виторг, то краще використовувати експоненціальний розподіл як найпростіше наближення.
2. Завести лічильник `bad_cnt`, який буде рахувати кількість разів, коли критерій помилився. Ініціалізувати його нулем.
3. Далі в циклі розміру $N$, де $N$ --- натуральне число від 1000 до нескінченності (чим воно більше, тим краще):
   - Симулюємо створення вибірки з розподілу, обраного на першому кроці. Так, щоб вірною була $H_0$. У випадку AB-тесту симулювати треба не одну вибірку, а дві: для тесту і контролю.
   - Досліджуємо на згенерованих даних критерій, що перевіряється.
   - Далі перевірити, чи критерій відкинув нульову гіпотезу. Якщо так, то збільшуємо `bad_cnt` на одиницю.
4. Порахувати частку помилок. Це буде ймовірність того, що критерій помиляється.
    - Якщо вона приблизно збігається з $\alpha$, то все добре.
    - Якщо вона менша за $\alpha$, то в принципі це адекватний критерій на практиці, просто він буде менш потужний, ніж критерій, що помиляється рівно у $\alpha$ відсотку випадків. Але на практиці варто перевірити: а теоретично така ситуація можлива? Чи це помилка в коді критерію?
    - Якщо критерій помиляється більше, ніж у $\alpha$, то значить він некоректний і ним не можна користуватися. Використовуючи такий критерій, ви будете помилятися частіше, ніж треба, і це може призвести до серйозних помилок у бізнес-рішеннях.
    
Розглянемо процедуру на прикладі: перевіримо, чи можна використовувати $t$-тест для вибірок із нормального розподілу?

```{python}
np.random.seed(42)

bad_cnt = 0
N = 10000
alpha = 0.05

sample_dist = norm(loc=2, scale=3)
mu0=sample_dist.expect()
for i in range(N):
    test    = sample_dist.rvs(5)
    control = sample_dist.rvs(5)
    pvalue = ttest_ind(test, control, alternative='two-sided').pvalue
    bad_cnt += (pvalue < alpha)

print(f"FPR: {bad_cnt/N:.3f}")
```

Зверніть увагу, що $FPR=$ `{python} np.round(bad_cnt/N, 2)`, хоча він мав дорівнювати 5%. Чи правда, що критерій некоректний? Ні, ми просто не врахували шум: ми навряд чи зможемо отримати на кінцевому числі експериментів точну рівність $\text{FPR} = \alpha$.

Тому пункт 4 процедури перевірки критерію можна уточнити:

4. Порахувати частку помилок й *побудувати довірчий інтервал для нього*. Якщо $\alpha$ лежить у ньому, значить усе добре, а інакше розбираємося, що пішло не так.

Довірчий інтервал можна побудувати різними способами ([див. -@sec-ci]). Але можна зробити простіше: у Python є функція, яка будує довірчий інтервал Вілсона[^Wilson]: він не такий точний, як ми виводили раніше, зате він швидший й працює швидше. Давайте спробуємо його реалізувати:

[^Wilson]: @Wilson1927

```{python}
ci = proportion_confint(count = bad_cnt, nobs = N, alpha=0.05, method='wilson')
print(f"FPR: {bad_cnt/N:.3f}\nДовірчий інтервал: ({ci[0]:.3f}, {ci[1]:.3f})")
```

Як бачимо, що 5% потрапили в довірчий інтервал, а отже, ми можемо вважати, що критерій є валідним для нашого завдання.

А що, якби розподіл був складнішим?

Розглянемо приклад, коли магматичне сподівання в тесті й контролі рівні, але вибірки з різних розподілів. Тобто $H_0$ правильна, але розподіли різні. Для цього ми можемо взяти два експоненціальних розподіли з різними параметрами. Наприклад, раніше в середньому виручка від користувача була приблизно 10 гривень, а після введення ефекту впливу (нове ціноутворення) частина користувачів стала менше платити, але середній чек залишився таким самим: 10 гривень.

```{python}
#| label: fig-t-test-2
#| fig-cap: "Приклад, коли $H_0$ правильна, але розподіли різні"

np.random.seed(42)

test_dist = expon(scale = 10)
control_dist = expon(loc=5, scale = 5)

x = np.linspace(0, 100, 1000)

plt.plot(x, test_dist.pdf(x), label='Тест', color=turquoise)
plt.plot(x, control_dist.pdf(x), label='Контроль', color=slate)
plt.xlabel('x')
plt.ylabel('Щільність')
plt.legend()
plt.grid(linewidth=0.2)
plt.show()
```

Напишемо функцію `check_criterion()`, яка буде перевіряти критерій на коректність. Вона приймає на вхід розподіли для тесту й контролю, розмір вибірки, кількість експериментів, а також вміє виводити довірчий інтервал.

```{python}
def check_criterion(test_dist, control_dist, sample_size, N_exps=10000, to_print=True):
    np.random.seed(35)
    bad_cnt=0
    alpha=0.05

    for i in range(N_exps):
        test    = test_dist.rvs(sample_size)
        control = control_dist.rvs(sample_size)
        pvalue = ttest_ind(test, control, equal_var=False, alternative='two-sided').pvalue
        bad_cnt += (pvalue < alpha)

    ci = proportion_confint(count = bad_cnt, nobs = N_exps, alpha=0.05, method='wilson')

    if to_print:
        print(f"FPR: {bad_cnt/N_exps:.3f}\nДовірчий інтервал: ({ci[0]:.3f}, {ci[1]:.3f})")
    else:
        return ci
```

Тепер перевіримо, чи працює $t$-тест для вибірок з різних експоненціальних розподілів. Одразу спробуємо перевірити відомий міф про достатність вибірки 30[^hogg]. Чи дійсно $t$-тест працює, якщо вибірка більша за 30?

[^hogg]: @Hogg2015

```{python}
check_criterion(test_dist, control_dist, sample_size=40)
```

Як бачимо, $t$-тест не спрацював, хоча вибірка більша за 30. Істинне $\alpha$ не лежить у довірчому інтервалі. Але з якого розміру вибірки $t$-тест почне працювати правильно?

## Визначення розміру вибірки {#sec-size}

Визначити розмір вибірки, з якого $t$-тест почне працювати, можна за допомогою методу Монте-Карло. Для цього ми будемо перевіряти $t$-тест на вибірках різного розміру, поки не знайдемо такий розмір, при якому $t$-тест почне працювати. Для цього ми будемо перевіряти $t$-тест на вибірках від 20 до 100 з кроком 10.

```{python}
scale = np.arange(20, 110, 10)
for N in scale:
    left, right = check_criterion(test_dist=test_dist, control_dist=control_dist, sample_size=N, N_exps=10000, to_print=False)
    if left < alpha < right:
        print(f"Розмір вибірки {N} достатній для $t$-тесту")
        break
```

Як бачимо, $t$-тест починає працювати з вибірки розміром 60.

```{python}
check_criterion(test_dist=test_dist, control_dist=control_dist, sample_size=60)
```