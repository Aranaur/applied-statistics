# $Z$-критерій Фішера {#sec-z-test}

```{python}
#| include: false

with open("_common.py") as f:
    exec(f.read())
```

У цьому розділі ми розглянемо $Z$-критерій Фішера, який використовується для перевірки гіпотез про середнє значення генеральної сукупності з відомою дисперсією.

Далі, для виведення критеріїв нам потрібен нормальний розподіл. *Потому що саме цьому розподілу підпорядковується середнє вибірок*. Тож давайте подивимося, що це взагалі таке, як з ним працювати в Python й які в нього є властивості.

## Нормальний розподіл {#sec-normal-distribution}

Нормальний розподіл $\mathcal{N}(\mu, \sigma^2)$ --- неперервний розподіл, у якому щільність спадає зі збільшенням відстані від математичного сподівання $\mu$ за швидкістю, пропорційною квадрату відстані (див. [формулу -@eq-normal-density]).

$$
f(x) = \frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^2},
$$ {#eq-normal-density}
де $x$ --- випадкова величина, $\mu$ --- математичне сподівання, $\sigma^2$ --- дисперсія.

На графіку нижче показано, як виглядає нормальний розподіл з різними параметрами $\mu$ та $\sigma^2$.

::: {#lst-normal-distribution}

```{python}
#| eval: false

x = np.linspace(-5, 5, 1000)
params = [(0, 1), (0, 2), (1, 1), (1, 2), (2, 1), (2, 2)]

for mu, sigma in params:
    plt.plot(x, norm.pdf(x, mu, sigma), label=f'μ={mu}, σ={sigma}')

plt.xlabel('x')
plt.ylabel('f(x)')
plt.legend()
plt.show()
```

Візуалізація нормального розподілу з різними параметрами $\mu$ та $\sigma^2$.
:::

```{python}
#| label: fig-normal-distribution
#| fig-cap: Нормальний розподіл з різними параметрами
#| echo: false

x = np.linspace(-5, 5, 1000)
params = [(0, 1), (0, 2), (1, 1), (1, 2), (2, 1), (2, 2)]

for mu, sigma in params:
    plt.plot(x, norm.pdf(x, mu, sigma), label=f'μ={mu}, σ={sigma}')

plt.xlabel('x')
plt.ylabel('f(x)')
plt.legend()
plt.show()
```

## Нормальний розподіл у Python {#sec-normal-distribution-python}

Нехай ми хочемо задати розподіл $\mathcal{N}(\mu, \sigma^2)$. Для цього є клас `norm`[^py-norm].

[^py-norm]: Документація доступна за посиланням <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.norm.html>.

Параметри класу:

- `loc` --- це $\mu$
- `scale` --- це $\sigma$, або **стандартне відхилення**. Не дисперсія!

Методи класу:

- `rvs()` --- згенерувати випадкові числа з розподілу $\mathcal{N}(\mu, \sigma^2)$
- `cdf(x)` --- кумулятивна функція розподілу (cumulative distribution function, CDF) в точці $x$, ймовірність того, що випадкова величина $X$ менша або дорівнює $x$.
- `ppf(q)` --- квантиль функції розподілу (percent-point function, PPF) для ймовірності $q$, ймовірність того, що випадкова величина $X$ менша або дорівнює $q$.
- `pdf(x)` --- щільність ймовірності (probability density function, PDF) в точці $x$, ймовірність того, що випадкова величина $X$ дорівнює $x$.

CDF та PPF --- це функції, які пов'язані між собою. CDF визначає ймовірність того, що випадкова величина $X$ менша або дорівнює $x$, а PPF визначає значення $x$, для якого ймовірність $X$ менша або дорівнює $q$.

Ініціалізуємо клас `norm` з параметрами $\mu = 0$ та $\sigma = 1$ (стандартний нормальний розподіл). Далі, згенеруємо випадкову вибірку з 50 спостережень, а також обчислимо PDF, CDF та PPF для $x = 1.5$.

::: {#lst-normal-distribution-2}

```{python}
std_norm = norm(loc=0, scale=1) # <1>

rnorm = std_norm.rvs(size=50, random_state=42) # <2>

CDF = std_norm.cdf(1.5) # <3>
PDF = std_norm.pdf(1.5) # <4>
PPF = std_norm.ppf(0.933) # <5>

display(
    Markdown(f"$P(X \\leq 1.5) = {CDF:.3f}$"), # <6>
    Markdown(f"$f(1.5) = {PDF:.3f}$"), # <7>
    Markdown(f"$z_{{0.933}} = \Phi^{{-1}}(0.933) = {PPF:.3f}$") # <8>
)
```
1. Ініціалізація класу `norm` з параметрами $\mu = 0$ та $\sigma = 1$.
2. Генерація випадкової вибірки з 50 спостережень.
3. Обчислення PDF для $x = 1.5$.
4. Обчислення CDF для $x = 1.5$.
5. Обчислення PPF для $q = 0.933$.
6. Ймовірність того, що випадкова величина $X$ менша або дорівнює $1.5$.
7. Ймовірність того, що випадкова величина $X$ дорівнює $1.5$.
8. Значення $x$, для якого ймовірність $X$ менша або дорівнює $0.933$.

Нормальний розподіл у Python.
:::

Візуалізація методів класу `norm` показана на [рисунку -@fig-normal-distribution-2].

```{python}
#| label: fig-normal-distribution-2
#| fig-cap: Демонстрація методів класу `norm`
#| echo: false

mu, sigma = 0, 1
q = 1.5


# Генерація випадкової вибірки
rnorm = norm.rvs(loc=mu, scale=sigma, size=50, random_state=42)

# Щільність розподілу (PDF)
x = np.linspace(-4, 4, 1000)
dnrom = norm.pdf(q, loc=mu, scale=sigma)
pnorm = norm.cdf(q, loc=mu, scale=sigma)
qnorm = norm.ppf(0.933, loc=mu, scale=sigma)

# PDF for x
plt.plot(x, norm.pdf(x, loc=mu, scale=sigma), label='PDF', color = turquoise)

# Random sample
plt.scatter(rnorm, np.random.normal(0, 0.005, size=rnorm.shape), color=red_pink, alpha=0.5, s=10)
plt.annotate('norm.rvs(50)', xy=(-1, 0), xytext=(-3, 0.2), color=red_pink, ha = 'center',
             arrowprops=dict(facecolor=red_pink, shrink=0.05))

# CDF for 2
plt.fill_between(x, norm.pdf(x, loc=mu, scale=sigma), where=(x <= q), color=turquoise, alpha=0.2)
plt.annotate('norm.cdf(1.5) = 0.933', xy=(1, 0.15), xytext=(1.5, 0.3), color=turquoise,
             arrowprops=dict(facecolor=turquoise, shrink=0.05))

# PDF for 2
plt.scatter(q, norm.pdf(q, loc=mu, scale=sigma), color=orange, s=15, zorder=5)
plt.plot([q, q], [0, dnrom], color=orange, linewidth=1, linestyle='--')
plt.annotate('norm.pdf(1.5) = 0.129', xy=(q, dnrom), xytext=(2, 0.2), color=orange, ha = 'left',
             arrowprops=dict(facecolor=orange, shrink=0.05))

# PPF for 0.977
plt.scatter(q, 0, color=purple, label='qnorm(0.977)', s=15, zorder=5)
plt.annotate('norm.ppf(0.933) = 1.5', xy=(q, 0), xytext=(2, 0.1), color=purple, ha = 'left',
             arrowprops=dict(facecolor=purple, shrink=0.05))


plt.xlabel('x')
plt.ylabel('Щільність')
plt.show()
```

## Властивості нормального розподілу {#sec-normal-distribution-properties}

Нормальний розподіл має кілька важливих властивостей[^normal-properties]:

[^normal-properties]: Доведення цих властивостей можна знайти в роботі @lemons2002.

1. **Сума двох незалежних нормально розподілених випадкових величин також має нормальний розподіл**:

$$
\begin{aligned}
\xi_1 &\sim \mathcal{N}(\mu_1, \sigma_1^2) \\
\xi_2 &\sim \mathcal{N}(\mu_2, \sigma_2^2) \\
\xi_1 + \xi_2 &\sim \mathcal{N}(\mu_1 + \mu_2, \sigma_1^2 + \sigma_2^2)
\end{aligned}
$$ {#eq-normal-sum}
де $\xi_1$ та $\xi_2$ --- незалежні нормально розподілені випадкові величини з параметрами $\mu_1$, $\sigma_1^2$ та $\mu_2$, $\sigma_2^2$ відповідно.

2. **Множення нормально розподіленої випадкової величини на константу також дає нормально розподілену величину**:

$$
a \xi_1 \sim \mathcal{N}(a\mu_1, a^2\sigma_1^2)
$$ {#eq-normal-multiply}
де $a$ --- константа, $\xi_1$ --- нормально розподілена випадкова величина з параметрами $\mu_1$, $\sigma_1^2$.

### Перевірка властивостей в Python {#sec-normal-distribution-properties-python}

За допомогою мови Python ми можемо перевірити ці властивості. Почнемо з @eq-normal-sum. Для цього ми згенеруємо дві нормально розподілені випадкові величини $\xi_1$ та $\xi_2$ з параметрами $\mu_1 = 0$, $\sigma_1^2 = 1$ та $\mu_2 = 1$, $\sigma_2^2 = 4$. Потім, ми обчислимо їхню суму та перевіримо, чи має вона нормальний розподіл з параметрами $\mu_1 + \mu_2$ та $\sigma_1^2 + \sigma_2^2$.

::: {#lst-normal-distribution-properties}
```{python}

mean_one, mean_two = 3, -1 # <1>
var_one, var_two = 4, 2 # <2>

n = 10000 # <3>

x1 = norm.rvs(loc=mean_one, scale=np.sqrt(var_one), size=n) # <4>
x2 = norm.rvs(loc=mean_two, scale=np.sqrt(var_two), size=n) # <4>

x_sum = x1 + x2 # <5>
check_sum = norm(loc=mean_one + mean_two, scale=np.sqrt(var_one + var_two)) # <6>

x_grid = np.linspace(-8, 12, 1000) # <7>

sns.displot(x_sum, kde=True, stat='density', color=red_pink, label='Емпіричний розподіл') # <8>
plt.plot(x_grid, check_sum.pdf(x_grid), color=turquoise, label='Теоретичний розподіл') # <9>
plt.xlabel('x')
plt.ylabel('Щільність')
plt.legend()
plt.show()
```
1. Параметри $\mu_1$ та $\mu_2$.
2. Параметри $\sigma_1^2$ та $\sigma_2^2$.
3. Кількість спостережень.
4. Генерація нормально розподілених випадкових величин $\xi_1$ та $\xi_2$.
5. Сума двох нормально розподілених випадкових величин.
6. Параметри суми $\xi_1 + \xi_2$.
7. Стандартне відхилення суми $\xi_1 + \xi_2$.
8. Емпіричний розподіл суми $\xi_1 + \xi_2$.
9. Теоретичний розподіл суми $\xi_1 + \xi_2$.

Візуалізація нормального розподілу суми двох нормально розподілених випадкових величин.
:::

Видно, що розподіли приблизно збіглися! А значить ми переконалися, що формула правильна.

Другу властивість @eq-normal-multiply можна перевірити аналогічно. Для цього ми згенеруємо нормально розподілену випадкову величину $\xi_1$ з параметрами $\mu_1 = 0$, $\sigma_1^2 = 1$ та помножимо її на константу $a = 2$. Потім, ми перевіримо, чи має вона нормальний розподіл з параметрами $a\mu_1$ та $a^2\sigma_1^2$.

::: {#lst-normal-distribution-properties-2}
```{python}
mean_one = 0 # <1>
var_one = 1 # <2>
a = 2 # <3>
n = 10000 # <4>
x1 = norm.rvs(loc=mean_one, scale=np.sqrt(var_one), size=n) # <5>
x_mult = a * x1 # <6>
check_mult = norm(loc=a * mean_one, scale=np.sqrt(a**2 * var_one)) # <7>
x_grid = np.linspace(-8, 8, 1000) # <8>

sns.displot(x_mult, kde=True, stat='density', color=turquoise, label='Емпіричний розподіл') # <9>
plt.plot(x_grid, check_mult.pdf(x_grid), color=red_pink, label='Теоретичний розподіл', alpha=0.8) # <9>
plt.xlabel('x') # <9>
plt.ylabel('Щільність') # <9>
plt.legend() # <9>
plt.show() # <9>
```
1. Параметри $\mu_1$ та $\sigma_1^2$.
2. Параметри $\sigma_1^2$.
3. Константа $a$.
4. Кількість спостережень.
5. Генерація нормально розподіленої випадкової величини $\xi_1$.
6. Множення нормально розподіленої випадкової величини $\xi_1$ на константу $a$.
7. Параметри множення $\xi_1$ на константу $a$.
8. Стандартне відхилення множення $\xi_1$ на константу $a$.
9. Емпіричний та теоретичний розподіл множення $\xi_1$ на константу $a$.

Візуалізація нормального розподілу множення нормально розподіленої випадкової величини на константу.
:::

Цього разу розподіли також збіглися. А значить ми переконалися, що формула правильна.

## Центральна гранична теорема {#sec-central-limit-theorem}

Для початку пригадаємо теорему, яка є основоположною теоремою для всіх критеріїв, які ми розглянемо найближчим часом.

::: {#thm-central-limit-theorem}

## Центральна гранична теорема, ЦГТ

Нехай $\xi_1, ..., \xi_n$ --- **незалежно** однаково розподілені випадкові величини, в яких існують математичне сподівання та дисперсія: $E [\xi_i] = \mu < \infty$ і $Var[\xi_i] = \sigma^2 < \infty$, тоді $\sqrt{n}\dfrac{\overline \xi - \mu}{\sqrt{\sigma^2}}$ збігається за розподілом[^convergence-in-distribution] до $\mathcal{N}(0, 1)$.

[^convergence-in-distribution]: Послідовність випадкових величин $\xi_n$ збігається за розподілом до $\xi$, позначаємо $\xi_n \xrightarrow{d} \xi$, якщо $\lim_{n \to \infty} F_{\xi_n}(x) = F_{\xi}(x)$ для всіх $x$, в яких $F_{\xi}(x)$ неперервна.

:::

Це означає, що якщо випадкові величини в експерименті **незалежні й однаково розподілені** й ваша вибірка **досить велика**, то можна вважати, що

$$
\sqrt{n}\dfrac{\overline \xi - \mu}{\sqrt{\sigma^2}} \sim \mathcal{N}(0, 1),
$$ {#eq-central-limit-theorem}
де $\overline \xi$ --- середнє арифметичне вибірки, $n$ --- кількість спостережень, $\mu$ --- математичне сподівання генеральної сукупності, $\sigma^2$ --- дисперсія генеральної сукупності.

::: {.callout-note}
Випадкові величини можуть бути слабко залежні одна від одної й злегка по-різному розподілені. Центральна гранична теорема все ще буде правильною, @Gnedenko2021.
:::

## Візуалізація ЦГТ {#sec-visualization-central-limit-theorem}

Щоб краще розуміти, як працює ЦГТ, я пропоную візуалізувати теорему: подивимося на розподіл середніх значень у різних вибірках. Як ми це зробимо?

- Щоб подивитися, що деяка випадкова величина з нормального розподілу, нам потрібна вибірка цих випадкових величин.
- У цьому випадку нам потрібна вибірка статистик із ЦГТ. Тому нам потрібно згенерувати $N$ вибірок по $M$ елементів у кожній.
    - По кожній вибірці треба порахувати середнє за $M$ елементами.
    - У підсумку ми отримаємо вибірку з $N$ елементів.
    - Вона і має бути з нормального розподілу.

::: {#lst-visualization-central-limit-theorem}
```{python}
def visualize_CLT(sample_generator, expected_value, variance):
    np.random.seed(42)
    N = 5000 # <1>
    clt_sample = [] # <2>
    for _ in range(N):
        sample = sample_generator() # <3>
        sample_size = len(sample) # <4>
        statistic = np.sqrt(sample_size) * (np.mean(sample) - expected_value) / np.sqrt(variance) # <5>
        clt_sample.append(statistic) # <6>

    x = np.linspace(-4, 4, 1000) # <7>
    sns.displot(clt_sample, kde=True, stat='density', color=turquoise, label='Емпіричний розподіл') # <7>
    plt.plot(x, norm().pdf(x), color=red_pink, label='$\mathcal{N}(0, 1)$', alpha=0.8) # <7>
    plt.legend() # <7>
    plt.xlabel('X') # <7>
    plt.ylabel('Щільність') # <7>
    plt.show() # <7>


p = 0.01
n = 20
size = 5000

visualize_CLT(lambda: np.random.binomial(n, p, size), # <8>
              expected_value = p * n, # <9>
              variance = n * p * (1 - p) # <10>
)
```

1. Кількість вибірок.
2. Пустий масив для зберігання статистик.
3. Генерація вибірки з $M$ елементами.
4. Кількість елементів у вибірці.
5. Обчислення статистики.
6. Додавання статистики до масиву.
7. Візуалізація емпіричного розподілу та теоретичного розподілу.
8. Генерація вибірки з біноміального розподілу.
9. Математичне сподівання біноміального розподілу.
10. Дисперсія біноміального розподілу.

Візуалізація ЦГТ при великій вибірці з біноміального розподілу.
:::

Емпірична щільність достатньо близько збігається з теоретичним розподілом. А що якщо зменшити вибірку, за якою рахується середнє?

::: {#lst-visualization-central-limit-theorem-2}
```{python}
p = 0.05
n = 20
size = 10

visualize_CLT(lambda: np.random.binomial(n, p, size),
              expected_value = p * n,
              variance = n * p * (1 - p)
)
```

Візуалізація ЦГТ при малій вибірці з біноміального розподілу.
:::